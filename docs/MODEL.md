# LLM Detector Model Training Documentation

This document provides a detailed explanation of how the LLM detector model is trained, including data sources, preprocessing, feature extraction, model architecture, and aggregation strategies.

## Overview

The LLM detector uses a **regularized logistic regression model** trained on sentence-level features to classify text as human-generated or LLM-generated. The system employs:

1. **Sentence-level classification**: Each sentence is classified independently
2. **Feature standardization**: Z-score normalization using StandardScaler
3. **Class balancing**: Handles imbalanced datasets via class weights
4. **Document aggregation**: Combines sentence predictions into document-level scores

## Training Pipeline Architecture

```
Data Sources → Sentence Extraction → Feature Extraction → Model Training → Evaluation
     ↓                                        ↓
  Baselines                            Feature Vectorization
```

## Data Sources

### Human Text Sources

The training pipeline streams data from three human text corpora:

1. **FinePDFs** (`fine_pdfs`)
   - Academic and technical PDF documents
   - High-quality, professionally edited content
   - Filtered for English text and minimum quality thresholds

2. **Project Gutenberg** (`gutenberg`)
   - Classic literature and historical texts
   - Public domain books
   - Natural human writing from various eras

3. **Wikipedia** (`wikipedia`)
   - Encyclopedia articles (20220301 dump)
   - Collaboratively edited content
   - Diverse topics and writing styles

### LLM Text Sources

Three LLM-generated text corpora:

1. **Cosmopedia** (`cosmopedia`)
   - Synthetic textbook content
   - Educational material generated by LLMs
   - Structured, explanatory text

2. **LMSYS Chat** (`lmsys_chat`)
   - Real LLM chat conversations
   - Assistant responses from various models
   - Conversational style

3. **UltraChat** (`ultrachat`)
   - Multi-turn dialogue dataset
   - Generated conversations
   - Question-answering format

### Data Streaming Architecture

```python
class BaseDataSource:
    def stream_samples(self, limit=None) -> Iterator[TextSample]:
        # Yields TextSample objects with metadata
        # Handles batching, caching, and error recovery
```

Key features:
- **Streaming processing**: No full dataset loading required
- **Batch configuration**: Configurable batch sizes (default: 100)
- **Progress tracking**: Optional progress bars
- **Metadata preservation**: Source, sample ID, and position tracking

## Preprocessing Pipeline

### 1. Sentence Segmentation

Uses the **nupunkt-rs** model for robust sentence boundary detection:

```python
def segment_sentences(text: str) -> list[str]:
    # Neural punctuation-based segmentation
    # Handles abbreviations, decimals, URLs
    # Minimum sentence length: 10 characters (configurable)
```

### 2. Text Normalization

- **Whitespace normalization**: Collapse multiple spaces
- **Unicode normalization**: Handle various encodings
- **Length filtering**: Skip sentences < min_length
- **Content validation**: Remove non-text content

### 3. Baseline Distribution Building

Before feature extraction, baseline distributions are computed from the human corpus:

```python
BaselineSet:
    unicode: (human_distribution, llm_distribution)  # Character frequencies
    regex: (human_distribution, llm_distribution)    # Token patterns
    punctws: (human_distribution, llm_distribution)  # Punctuation/whitespace
```

These baselines are used for divergence features, providing corpus-specific reference distributions.

## Feature Extraction Process

### Feature Vectorization Pipeline

```python
class FeatureVectorizer:
    def __init__(self, registry, scale_invariant_only=True, baseline_overrides=None):
        # registry: Feature definitions
        # scale_invariant_only: Use only length-independent features
        # baseline_overrides: Divergence feature baselines
```

### Feature Computation Flow

1. **Text Input**: Single sentence (10+ characters)

2. **Statistical Features** (28 features):
   ```python
   # Computed directly from text
   - Lexical diversity (TTR, MATTR, Herdan's C)
   - Length metrics (word/sentence length, CV)
   - Character distributions (case, digits, punctuation)
   - Linguistic patterns (function words, capitalization)
   ```

3. **Divergence Features** (3 features):
   ```python
   # Jensen-Shannon divergence from baselines
   - Character distribution vs. human baseline
   - Punctuation distribution vs. human baseline
   - Regex pattern distribution vs. human baseline
   ```

4. **Tokenizer Features** (50 features):
   ```python
   # For each tokenizer (GPT-2, BERT, RoBERTa, GPT-OSS-20B):
   - Efficiency metrics (chars/token)
   - Token characteristics (length, diversity)
   - Subword patterns
   # Cross-tokenizer metrics:
   - Efficiency variance
   - Efficiency standard deviation
   ```

### Feature Vector Assembly

Total feature vector: **81 dimensions** (default configuration)

```python
feature_vector = [
    *statistical_features,      # 28 dims
    *divergence_features,       # 3 dims
    *tokenizer_features,        # 48 dims
    *aggregated_tokenizer_features  # 2 dims
]
```

## Model Training

### Logistic Regression Configuration

```python
LogisticRegression(
    class_weight='balanced',  # Handle class imbalance
    max_iter=1000,           # Maximum iterations
    random_state=42,         # Reproducibility
    solver='lbfgs',          # Default optimization algorithm
    penalty='l2',            # L2 regularization (Ridge)
    C=1.0                    # Inverse regularization strength
)
```

### Training Process

1. **Data Preparation**:
   ```python
   # Build dataset from registry
   dataset = build_dataset_from_registry(
       registry=source_registry,
       vectorizer=feature_vectorizer,
       samples_per_source=10000,  # Configurable
       balance=True,              # Equal samples per class
       shuffle=True,              # Random ordering
       seed=42                    # Reproducibility
   )
   ```

2. **Feature Standardization**:
   ```python
   scaler = StandardScaler()
   X_scaled = scaler.fit_transform(X_train)
   # Transforms features to zero mean, unit variance
   ```

3. **Model Fitting**:
   ```python
   model.fit(X_scaled, y_train)
   # Binary classification: 0=human, 1=LLM
   ```

4. **Probability Calibration**:
   - Logistic regression provides well-calibrated probabilities
   - Output: P(LLM|features) directly usable

### Training Dataset Structure

```python
class FeatureDataset:
    matrix: list[list[float]]  # Shape: (n_samples, n_features)
    labels: list[int]          # 0=human, 1=LLM
    feature_names: list[str]   # Feature identifiers
    metadata: list[dict]       # Source, sample_id, etc.
```

### Train/Test Split

```python
if test_ratio:  # Default: 0.1 (10% test)
    train_set, test_set = dataset.split(test_ratio, seed=seed)
```

## Model Evaluation

### Sentence-Level Metrics

```python
metrics = {
    'accuracy': accuracy_score(y_true, y_pred),
    'precision': precision_score(y_true, y_pred),
    'recall': recall_score(y_true, y_pred),
    'f1': f1_score(y_true, y_pred)
}
```

### Document-Level Aggregation

The model aggregates sentence-level predictions to document-level scores using multiple strategies:

#### 1. **Logit-Weighted Mean** (Primary)
```python
def logit_weighted_mean(scores, weights):
    # Convert probabilities to logits
    logits = [log(p/(1-p)) for p in scores]
    # Weighted average in logit space
    avg_logit = weighted_mean(logits, weights)
    # Convert back to probability
    return 1 / (1 + exp(-avg_logit))
```
- **Advantages**: Handles extreme probabilities well
- **Use case**: Default aggregation method

#### 2. **Simple Mean**
```python
def simple_mean(scores):
    return sum(scores) / len(scores)
```
- **Advantages**: Intuitive, unweighted
- **Use case**: Diagnostic comparison

#### 3. **Length-Weighted Mean**
```python
def length_weighted_mean(scores, lengths):
    # Weight by sentence length (characters/words)
    return weighted_mean(scores, lengths)
```
- **Advantages**: Emphasizes longer sentences
- **Use case**: Academic/technical texts

#### 4. **Vote Fraction**
```python
def vote_fraction(scores, threshold=0.7):
    # Fraction of sentences above threshold
    return sum(1 for s in scores if s >= threshold) / len(scores)
```
- **Advantages**: Robust to outliers
- **Use case**: Mixed-content documents

#### 5. **Max Score**
```python
def max_score(scores):
    return max(scores)
```
- **Advantages**: Detects any LLM content
- **Use case**: Contamination detection

#### 6. **Trimmed Mean**
```python
def trimmed_mean(scores, trim_ratio=0.1):
    # Remove top/bottom 10% before averaging
    sorted_scores = sorted(scores)
    trim_count = int(len(scores) * trim_ratio)
    return mean(sorted_scores[trim_count:-trim_count])
```
- **Advantages**: Robust to outliers
- **Use case**: Noisy documents

#### 7. **Median**
```python
def median(scores):
    return sorted(scores)[len(scores)//2]
```
- **Advantages**: Robust central tendency
- **Use case**: Highly variable texts

## Model Persistence

### Saved Artifacts

1. **Model file** (`model.json.gz`):
   ```json
   {
       "format": "llm-detector/logistic-regression",
       "version": 1,
       "feature_names": ["feature_a", ...],
       "class_weight": "balanced",
       "max_iter": 1000,
       "random_state": 42,
       "scaler": {
           "mean": [0.12, ...],
            "scale": [0.98, ...]
       },
       "model": {
           "coefficients": [1.23, ...],
           "intercept": -0.45,
           "classes": [0, 1]
       }
   }
   ```

2. **Baselines file** (`baselines.json.gz`):
   ```json
   {
       "unicode": {
           "human": {"distribution": [...], "vocabulary": [...]},
           "llm": {"distribution": [...], "vocabulary": [...]}
       },
       "regex": {...},
       "punctws": {...}
   }
   ```

### Loading and Inference

```python
# Load model
model = LogisticRegressionModel.load('model.json.gz')
baselines = BaselineCache.load('baselines.json.gz')

# Create runtime
runtime = DetectorRuntime(
    model_path='model.json.gz',
    baseline_path='baselines.json.gz'
)

# Predict
result = runtime.predict("Text to classify")
# Returns: DetectionResult with probabilities and metadata
```

## Performance Characteristics

### Computational Complexity

- **Feature extraction**: O(n) for text length n
- **Tokenization**: O(n) per tokenizer (parallelizable)
- **Prediction**: O(f) for f features (81 dims)
- **Aggregation**: O(s) for s sentences

### Memory Requirements

- **Model size**: ~500KB (compressed)
- **Baselines**: ~1MB (compressed)
- **Runtime memory**: ~50MB (with tokenizers loaded)

### Inference Speed

- **Single sentence**: ~5-10ms
- **Document (100 sentences)**: ~500ms
- **Batch processing**: ~1000 docs/second

## Hyperparameter Tuning

### Key Parameters

1. **Minimum sentence length**: 10-50 characters
2. **Class weight**: 'balanced' or custom ratios
3. **Regularization (C)**: 0.1-10.0 (inverse strength)
4. **Feature selection**: Scale-invariant vs. all features
5. **Aggregation method**: Logit-weighted vs. alternatives

### Optimization Strategies

```python
# Grid search example
param_grid = {
    'C': [0.1, 1.0, 10.0],
    'class_weight': ['balanced', {0: 1, 1: 2}],
    'max_iter': [500, 1000, 2000]
}

# Cross-validation
cv_scores = cross_val_score(model, X, y, cv=5, scoring='f1')
```

## Limitations and Considerations

### Known Limitations

1. **Language**: Optimized for English text only
2. **Length dependency**: Best with 3+ sentences
3. **Domain shift**: May need retraining for specialized domains
4. **Adversarial robustness**: Not designed for adversarial inputs
5. **Model evolution**: Requires updates as LLMs evolve

### Best Practices

1. **Data quality**: Ensure clean, representative training data
2. **Feature engineering**: Consider domain-specific features
3. **Evaluation**: Use held-out test sets from target domain
4. **Monitoring**: Track performance degradation over time
5. **Updates**: Retrain periodically with new LLM outputs

## Advanced Customization

### Custom Feature Addition

```python
# Define new feature
def custom_feature(text: str) -> float:
    return compute_metric(text)

# Register with feature registry
registry.register(FeatureDefinition(
    name="custom.my_feature",
    category=FeatureCategory.STATISTICAL,
    compute_fn=custom_feature,
    scale_invariant=True
))
```

### Custom Data Sources

```python
class CustomSource(BaseDataSource):
    def stream_samples(self, limit=None):
        # Implement custom data streaming
        for text in self.load_texts():
            yield TextSample(
                text=text,
                source="custom",
                metadata={}
            )
```

### Alternative Models

While logistic regression is the default, the architecture supports:

```python
# Random Forest
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100)

# Gradient Boosting
from sklearn.ensemble import GradientBoostingClassifier
model = GradientBoostingClassifier()

# Neural Network
from sklearn.neural_network import MLPClassifier
model = MLPClassifier(hidden_layer_sizes=(100, 50))
```

## References

1. **Logistic Regression**: Hosmer, D. W., Lemeshow, S. (2000). Applied Logistic Regression.
2. **Feature Standardization**: Grus, J. (2019). Data Science from Scratch.
3. **Class Imbalance**: He, H., Garcia, E. A. (2009). Learning from Imbalanced Data.
4. **Sentence Segmentation**: Kiss, T., Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection.
